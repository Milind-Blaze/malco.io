[{"authors":["admin"],"categories":null,"content":"Malcolm Barrett is a PhD student in Epidemiology at the University of Southern California. He has worked on both mature and nascent cohort studies, as well as in several clinical settings. He served for two years in AmeriCorps at Federally-Qualified Health Centers and is committed to science as service.\n","date":1554595200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Malcolm Barrett is a PhD student in Epidemiology at the University of Southern California. He has worked on both mature and nascent cohort studies, as well as in several clinical settings. He served for two years in AmeriCorps at Federally-Qualified Health Centers and is committed to science as service.","tags":null,"title":"Malcolm Barrett","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":["r"],"content":"\n\n\n\n","date":1569522600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569522600,"objectID":"4cd7f4d3bbefd0044ba3e12dfd2b3e14","permalink":"/talk/functional-programming-with-purrr/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/talk/functional-programming-with-purrr/","section":"talk","summary":"One of R’s most powerful features is that it’s a functional programming language. purrr is a consistent and efficient toolkit for programming with functions and working with lists. At its heart is `map()` and friends: functions for the common pattern of iterating over a vector or list, doing something to each element, and then storing the results. These functions let you iterate efficiently and with less code. As Jenny Bryan once said, “of course someone needs to write `for` loops; it doesn’t have to be you.","tags":[],"title":"Functional programming with purrr","type":"talk"},{"authors":[],"categories":["r","dataviz","releases","causalinference"],"content":" I’m please to announce that ggdag 0.2.0 is now on CRAN! ggdag links the dagitty package, which contains powerful algorithms for analyzing causal DAGs, with the unlimited flexibility of ggplot2. ggdag coverts dagitty objects to a tidy DAG data structure, which allows you to both analyze your DAG and plot it easily in ggplot2. Let’s look at an example for a causal diagram of the effect of smoking on cardiac arrest.\nlibrary(ggdag) smoking_ca_dag \u0026lt;- dagify(cardiacarrest ~ cholesterol, cholesterol ~ smoking + weight, smoking ~ unhealthy, weight ~ unhealthy, labels = c(\u0026quot;cardiacarrest\u0026quot; = \u0026quot;Cardiac\\n Arrest\u0026quot;, \u0026quot;smoking\u0026quot; = \u0026quot;Smoking\u0026quot;, \u0026quot;cholesterol\u0026quot; = \u0026quot;Cholesterol\u0026quot;, \u0026quot;unhealthy\u0026quot; = \u0026quot;Unhealthy\\n Lifestyle\u0026quot;, \u0026quot;weight\u0026quot; = \u0026quot;Weight\u0026quot;), latent = \u0026quot;unhealthy\u0026quot;, exposure = \u0026quot;smoking\u0026quot;, outcome = \u0026quot;cardiacarrest\u0026quot;) %\u0026gt;% tidy_dagitty() smoking_ca_dag ## # A DAG with 5 nodes and 5 edges ## # ## # Exposure: smoking ## # Outcome: cardiacarrest ## # Latent Variable: unhealthy ## # ## # A tibble: 6 x 9 ## name x y direction to xend yend circular label ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;chr\u0026gt; ## 1 choleste… 13.2 11.8 -\u0026gt; cardiac… 12.0 11.2 FALSE Cholesterol ## 2 smoking 14.5 11.7 -\u0026gt; cholest… 13.2 11.8 FALSE Smoking ## 3 unhealthy 15.0 12.8 -\u0026gt; smoking 14.5 11.7 FALSE \u0026quot;Unhealthy… ## 4 unhealthy 15.0 12.8 -\u0026gt; weight 13.8 13.0 FALSE \u0026quot;Unhealthy… ## 5 weight 13.8 13.0 -\u0026gt; cholest… 13.2 11.8 FALSE Weight ## 6 cardiaca… 12.0 11.2 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; NA NA FALSE \u0026quot;Cardiac\\n… The tidy DAG structure looks like a tibble. ggdag 0.2.0 also prints some information about the DAG at the top.\nggdag(smoking_ca_dag, text = FALSE, use_labels = \u0026quot;label\u0026quot;) Here, smoking does increase the risk of cardiac arrest, but it’s also confounded by an unmeasured variable, a tendency towards an unhealthy lifestyle. That means that there are two open paths from smoking to cardiac arrest: the causal path through cholesterol and the backdoor path through weight. (This DAG is probably not quite right, because smoking also affects weight, but we’ll leave it as is for demonstration purposes.)\nIf you used ggdag 0.1.0, you may notice a big difference here: ggdag plots now look a lot more like base ggplot2 plots. While this has been the case in the development version for some time, one of the bigger mistakes in the initial release of ggdag was too much out-of-box customization. ggdag now does a much better job getting out of the way of ggplot2’s incredible system for aesthetics and themes. Let’s analyze the paths in the smoking DAG but take advantage of tools from ggplot2 to customize the plot.\nggdag_paths(smoking_ca_dag, text = FALSE, use_labels = \u0026quot;label\u0026quot;, shadow = TRUE) + theme_dag(base_size = 14) + theme(legend.position = \u0026quot;none\u0026quot;, strip.text = element_blank()) + # set node aesthetics scale_color_manual(values = \u0026quot;#0072B2\u0026quot;, na.value = \u0026quot;grey80\u0026quot;) + # set label aesthetics scale_fill_manual(values = \u0026quot;#0072B2\u0026quot;, na.value = \u0026quot;grey80\u0026quot;) + # set arrow aesthetics ggraph::scale_edge_color_manual(values = \u0026quot;#0072B2\u0026quot;, na.value = \u0026quot;grey80\u0026quot;) + ggtitle(\u0026quot;Open paths from smoking to cardiac arrest\u0026quot;) There are also many new themes available, each of which is prefixed with theme_dag*().\nLearn more about ggdag on the package website. There you’ll find articles on introducing ggdag, introducing DAGs, and discussing common structures of bias\nWhat else is new? This release ensures compatibility with ggraph 2.0.0 and also fixes a number of bugs (see the news section of the pkgdown site). In addition to better support for ggplot2 aesthetic functions, ggdag also now has better support for working directly in tidygraph/ggraph. ggraph is essential to ggdag’s geoms, but you might prefer to work with the full toolkit from that package.\nlibrary(tidygraph) library(ggraph) tblgraph_dag \u0026lt;- as_tbl_graph(smoking_ca_dag) tblgraph_dag ## # A tbl_graph: 5 nodes and 5 edges ## # ## # A directed acyclic simple graph with 1 component ## # ## # Node Data: 5 x 1 (active) ## name ## \u0026lt;chr\u0026gt; ## 1 cholesterol ## 2 smoking ## 3 unhealthy ## 4 weight ## 5 cardiacarrest ## # ## # Edge Data: 5 x 9 ## from to x y direction xend yend circular label ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 5 13.2 11.8 -\u0026gt; 12.0 11.2 FALSE Cholesterol ## 2 2 1 14.5 11.7 -\u0026gt; 13.2 11.8 FALSE Smoking ## 3 3 2 15.0 12.8 -\u0026gt; 14.5 11.7 FALSE \u0026quot;Unhealthy\\n Life… ## # … with 2 more rows tblgraph_dag %\u0026gt;% ggraph() + geom_node_text(aes(label = name)) + geom_edge_link(aes( start_cap = label_rect(node1.name), end_cap = label_rect(node2.name) ), arrow = arrow()) + theme_graph() tidygraph is designed to work with network data rather than causal diagrams, so many of the features are not as useful for causal DAGs as the algorithms from dagitty. However, tidygraph and ggraph have many tools for manipulating network-like data that are very powerful.\n Miss the old look? A lot has changed in the look of ggdag, but the old style hasn’t gone away. You can set the old theme with theme_dag_gray() and set the stylized nodes with geom_dag_node() (instead of geom_dag_point()) or with the stylized argument in the quick plotting functions.\nggdag(confounder_triangle(), stylized = TRUE) + theme_dag_gray()  ","date":1568678400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568678400,"objectID":"9cea47b9356b2c4626a2dd2050922af0","permalink":"/2019/09/17/tidy-causal-dags-with-ggdag-0-2-0/","publishdate":"2019-09-17T00:00:00Z","relpermalink":"/2019/09/17/tidy-causal-dags-with-ggdag-0-2-0/","section":"post","summary":"I’m please to announce that ggdag 0.2.0 is now on CRAN! ggdag links the dagitty package, which contains powerful algorithms for analyzing causal DAGs, with the unlimited flexibility of ggplot2. ggdag coverts dagitty objects to a tidy DAG data structure, which allows you to both analyze your DAG and plot it easily in ggplot2. Let’s look at an example for a causal diagram of the effect of smoking on cardiac arrest.","tags":["ggdag"],"title":"Tidy causal DAGs with ggdag 0.2.0","type":"post"},{"authors":[],"categories":["r"],"content":" The annual meeting of the Society for Epidemiologic Research (SER) took place June 18-21. The past two years, I’ve collected Twitter data (2018, 2019). The data were collected with the excellent rtweet package, and the data collection code was based on related code by Mike Kearney, the author of rtweet.\nSetup # for everything else :) library(tidyverse) # for tidy eval library(rlang) # for labeling tweets in plots library(ggrepel) # for network graphs library(ggraph) library(tidygraph) # for text analysis library(tidytext) Since the data were collected over several days, I’m going to read the saved data straight from GitHub. You’ll note that there is also some code to turn the tweets into network data (who is tweeting whom); I’ll also read that in.\nser_tweets \u0026lt;- read_rds(url(\u0026quot;https://github.com/malcolmbarrett/ser_conf_2019/blob/master/data/search.rds?raw=true\u0026quot;)) ser_tweets_2018 \u0026lt;- read_rds(url(\u0026quot;https://github.com/malcolmbarrett/ser_tweets/blob/master/data/search.rds?raw=true\u0026quot;)) twitter_graph \u0026lt;- readr::read_rds(url(\u0026quot;https://github.com/malcolmbarrett/ser_conf_2019/raw/master/data/twitter_graph.rds\u0026quot;)) # simplify tweet tibble ser_tweets \u0026lt;- ser_tweets %\u0026gt;% select(screen_name, text, created_at, favorite_count, retweet_count, is_retweet, hashtags, media_type) First, a few functions to help us out: plot_barchat() to plot horizontal bar charts, plot_tweets() to make a scatterplot labeled with the tweet text, and plot_screen_names() to quickly count number of times a screen name appeared and plot the top ten with plot_barchat().\n# plot horizontal bar charts plot_barchat \u0026lt;- function(.df, y) { y \u0026lt;- enquo(y) .df %\u0026gt;% # put the bars in order of their appearance mutate(!!y := fct_rev(fct_inorder(!!y))) %\u0026gt;% # y first because we\u0026#39;re flipping ggplot(aes(y = n, x = !!y)) + geom_col(color = NA, fill = \u0026quot;#0172B1\u0026quot;) + theme_minimal(14) + theme( panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(), panel.grid.minor.x = element_blank() ) + coord_flip() } # plot points labelled with tweet text plot_tweets \u0026lt;- function(.df, y) { y \u0026lt;- enquo(y) .df %\u0026gt;% mutate(text = str_wrap(text, 50)) %\u0026gt;% ggplot(aes(x = fct_inorder(screen_name), y = !!y)) + geom_point(color = \u0026quot;#0172B1\u0026quot;, size = 2) + geom_label_repel( aes(label = text), size = 2.5, force = 4, nudge_y = 3, point.padding = .5 ) + theme_minimal(14) } # group by screen name, count, and plot 1st 10 plot_screen_name \u0026lt;- function(.df) { .df %\u0026gt;% group_by(screen_name) %\u0026gt;% summarize(n = n()) %\u0026gt;% mutate(screen_name = paste0(\u0026quot;@\u0026quot;, screen_name)) %\u0026gt;% arrange(desc(n)) %\u0026gt;% top_n(10) %\u0026gt;% plot_barchat(screen_name) + xlab(element_blank()) }  Tweets vs. 2018 I noticed this almost immediately: there were many more tweets this year. Between the 2018 and 2019, there was tremendous growth in the epidemiology Twitter community under the hashtag #epitwitter, and it showed. There were about twice as many original tweets and retweets during the official days of the conference.\nn_2019 \u0026lt;- ser_tweets %\u0026gt;% filter(between(created_at, as.POSIXct(\u0026quot;2019-06-18\u0026quot;), as.POSIXct(\u0026quot;2019-06-21\u0026quot;))) %\u0026gt;% group_by(is_retweet) %\u0026gt;% summarize(n = n()) %\u0026gt;% mutate(year = \u0026quot;2019\u0026quot;) n_2018 \u0026lt;- ser_tweets_2018 %\u0026gt;% filter(between(created_at, as.POSIXct(\u0026quot;2018-06-19\u0026quot;), as.POSIXct(\u0026quot;2018-06-22\u0026quot;))) %\u0026gt;% group_by(is_retweet) %\u0026gt;% summarize(n = n()) %\u0026gt;% mutate(year = \u0026quot;2018\u0026quot;) label_retweet \u0026lt;- function(x) ifelse(x, \u0026quot;retweet\u0026quot;, \u0026quot;original tweet\u0026quot;) bind_rows(n_2019, n_2018) %\u0026gt;% plot_barchat(year) + facet_wrap(~is_retweet, labeller = as_labeller(label_retweet)) Twitter connections and network centrality 1,026 users tweeted or retweeted a tweet with the #SER2019 hashtag. Many were single tweets, but there were hundreds of people interacting. From the 2019 repo:\nknitr::include_graphics(\u0026quot;https://raw.githubusercontent.com/malcolmbarrett/ser_conf_2019/master/tweet_network.png\u0026quot;) A big reason for the growth in #EpiTwitter is the community building that Ellie Murray, aka @EpiEllie, has done. Unsurprisingly, she is the most central person in the Twitter graph, followed by the SER twitter account.\ntwitter_graph %\u0026gt;% activate(nodes) %\u0026gt;% arrange(desc(value)) %\u0026gt;% as_tibble() %\u0026gt;% top_n(10) %\u0026gt;% mutate(name = paste0(\u0026quot;@\u0026quot;, name), n = value) %\u0026gt;% plot_barchat(name) + labs(y = \u0026quot;centrality\u0026quot;, x = element_blank())  Subjects by day A few months back, I read Tidy Text Mining with R, and since then, I’ve just been waiting to analyze word use by day of the conference. I won’t go over this code in detail, but it uses tf-dif to analyze words that were unique to each day. You can find a discussion and more examples of this approach in the book.\ntfidf_tweets \u0026lt;- ser_tweets %\u0026gt;% filter( !is_retweet, between(created_at, as.POSIXct(\u0026quot;2019-06-18\u0026quot;), as.POSIXct(\u0026quot;2019-06-21\u0026quot;)) ) %\u0026gt;% # tidytext doesn\u0026#39;t like these list columns select_if(~ !is_list(.x)) %\u0026gt;% # create day of tweet mutate(day = lubridate::wday(created_at, label = TRUE)) %\u0026gt;% # unnest by word and count each word by day unnest_tokens(word, text) %\u0026gt;% group_by(day) %\u0026gt;% count(word, sort = TRUE) %\u0026gt;% ungroup() %\u0026gt;% # add tf-idf and sort bind_tf_idf(word, day, n) %\u0026gt;% arrange(desc(tf_idf)) %\u0026gt;% mutate(word = fct_rev(fct_inorder(word))) %\u0026gt;% # pick top 10 by day group_by(day) %\u0026gt;% top_n(10) %\u0026gt;% ungroup() So, what was unique to each day? Tuesday was full of workshops, including workshops on R, g-computation, and E-values. Wednesday started with a fantastic keynote by Alix Spiegel of Invisibilia, which was like a live version of the podcast. People also talked about Dr. Shawnita Sealy-Jefferson’s powerful talk, “Listen to and trust black women.” Thursday began with a debate by Sandro Galea and Miguel Hernán about the role of causal inference in social epidemiology. The same day, people were talking about race and racism in relation to health epidemiology. On the final day of the conference, people were talking about the reunion parties the night before, #blackgirlmagic, and Dr. Sharrelle Barber.\ntfidf_tweets %\u0026gt;% ggplot(aes(word, tf_idf)) + geom_col(color = NA, fill = \u0026quot;#0172B1\u0026quot;) + theme_minimal(14) + theme( panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(), panel.grid.minor.x = element_blank() ) + coord_flip() + facet_wrap(~ day, scales = \u0026quot;free\u0026quot;)  Other active hashtags What other hashtags were used with #SER2019? Unsurprisingly, #EpiTwitter leads the list, followed by #SPER2019, related to the SPER meeting that happens immediately before SER. People were also talking about the better poster design, open science, causal inference, Lisa Bodnar’s annual SER party, and more.\nunnested_hashtags \u0026lt;- ser_tweets %\u0026gt;% filter(!is_retweet) %\u0026gt;% unnest(hashtags) %\u0026gt;% mutate_at(vars(hashtags), tolower) %\u0026gt;% filter( stringr::str_detect( hashtags, stringr::fixed(\u0026quot;SER2019\u0026quot;, ignore_case = TRUE), negate = TRUE ) ) unnested_hashtags %\u0026gt;% group_by(hashtags) %\u0026gt;% summarize(n = n()) %\u0026gt;% arrange(desc(n)) %\u0026gt;% top_n(10) %\u0026gt;% plot_barchat(hashtags)  BlackEpiMatters One particularly powerful event was the inception of the #BlackEpiMatters hashtag, which lead to the creation of the @black_epi account. There were many incredible talks by black epidemiologists at this meeting, and the fact that the rooms were absolutely packed for these talks magnified the fact that most were in smaller rooms. Many people called out SER for this, and they have promised to address it for SER 2020. Additionally, many black epidemiologists who are active on Twitter will be doing Twitter takeovers of the SER account over the next year.\nSo who were the people using #blackepimatters during the meeting? Sharrelle Barber lead the way.\nunnested_hashtags %\u0026gt;% filter( hashtags %in% c(\u0026quot;blackepimatters\u0026quot;, \u0026quot;blackepidemiologymatters\u0026quot;) ) %\u0026gt;% plot_screen_name() Finally, how about a few rankings?\n Most retweets My poster got the most retweets 😱😱😱. Also popular were Jess Rohmann’s list of DAG papers and the “Beyond ‘Is Race a Cause?’” session, which was standing-room with a line out the door (I know this because I couldn’t get in the door!).\nser_tweets %\u0026gt;% filter(!is_retweet) %\u0026gt;% arrange(desc(retweet_count)) %\u0026gt;% top_n(5, retweet_count) %\u0026gt;% plot_tweets(retweet_count) + labs(x = \u0026quot;twitter user\u0026quot;, y = \u0026quot;n retweets\u0026quot;)  Most favorites My poster also got the most favorites 😱😱😱. One high-impact tweet was Bill Miller’s offer to mentor attendees. I barely had a chance to say hi to him he was so busy! People also liked Ellie’s temporary tattoos, #SER2019 bingo, and the #betterposter design.\nser_tweets %\u0026gt;% filter(!is_retweet) %\u0026gt;% arrange(desc(favorite_count)) %\u0026gt;% top_n(5, favorite_count) %\u0026gt;% plot_tweets(favorite_count) + labs(x = \u0026quot;twitter user\u0026quot;, y = \u0026quot;n favorites\u0026quot;)  Most tweets @AbbyCScience (taking a well-deserved Twitter break) was responsible for the most original tweets by a long shot.\n# number original tweets ser_tweets %\u0026gt;% filter(!is_retweet) %\u0026gt;% plot_screen_name()  Most retweets Unsurprisingly, the most retweets came from my EpiBot, which retweets #EpiTwitter. But Ellie is a surprisingly close second!\n# number retweets ser_tweets %\u0026gt;% filter(is_retweet) %\u0026gt;% plot_screen_name()  Most photos posted Peter Tennant posted the most photos (plus, there’s a video of his poster presentation).\nser_tweets %\u0026gt;% filter(!is_retweet) %\u0026gt;% unnest(media_type) %\u0026gt;% filter(media_type == \u0026quot;photo\u0026quot;) %\u0026gt;% plot_screen_name() And that’s it for #SER2019!\n  ","date":1561248000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561248000,"objectID":"d5f4471ad419931d36d8fee46bbe8bd4","permalink":"/2019/06/23/ser2019-in-review/","publishdate":"2019-06-23T00:00:00Z","relpermalink":"/2019/06/23/ser2019-in-review/","section":"post","summary":"The annual meeting of the Society for Epidemiologic Research (SER) took place June 18-21. The past two years, I’ve collected Twitter data (2018, 2019). The data were collected with the excellent rtweet package, and the data collection code was based on related code by Mike Kearney, the author of rtweet.\nSetup # for everything else :) library(tidyverse) # for tidy eval library(rlang) # for labeling tweets in plots library(ggrepel) # for network graphs library(ggraph) library(tidygraph) # for text analysis library(tidytext) Since the data were collected over several days, I’m going to read the saved data straight from GitHub.","tags":["ser","textanalysis"],"title":"#SER2019 In Review","type":"post"},{"authors":[],"categories":null,"content":"","date":1560844800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560844800,"objectID":"adb15405b95fe373ecbd70b5eda5d3c7","permalink":"/talk/2019-06-18-ser-workshop/","publishdate":"2019-10-06T21:14:04-07:00","relpermalink":"/talk/2019-06-18-ser-workshop/","section":"talk","summary":"Recent developments by the R community have revolutionized the data analysis pipeline in R, from manipulating and visualizing data to communicating results. Our workshop will provide hands-on training in tools from the tidyverse ecosystem, using real epidemiologic data. In the first section, we will teach data manipulation with dplyr, a package that makes data cleaning easy, flexible, and enjoyable. In the next section, we will teach data visualization with ggplot2, the most popular plotting package in R, with a focus on creating publication-quality plots. We will then put these tools together to make reproducible documents. Using R Markdown, we will weave code and text together and learn to write papers and reports, exported to PDF, Word, or HTML, entirely in R. This workflow easily propagates upstream changes to data or analyses throughout a document and eliminates copy and paste errors. Together, these tools form a data analysis pipeline for reproducible, publication-ready work.","tags":[],"title":"Data manipulation, visualization, and reproducible documents with R and the Tidyverse","type":"talk"},{"authors":[],"categories":["r","releases"],"content":" I’m pleased to announce the CRAN release of partition 0.1.0. partition is a fast and flexible data reduction framework that minimizes information loss and creates interpretable clusters. partition uses agglomorative clustering: it starts from the ground up, matching pairs of variables and assessing the amount of information that would be explained by their reduction. If the information is above this user-specified threshold, the data is reduced. This type of reduction is particularly useful in very redundant data, such as high-resolution genetic data.\nCreating partitions partition() takes a data frame and reduces it as much as possible without creating clusters below the minimum amount of information specified in the threshold argument.\n# install.packages(\u0026quot;partition\u0026quot;) library(partition) set.seed(1234) # simulate correlated data df \u0026lt;- simulate_block_data(c(3, 4, 5), lower_corr = .4, upper_corr = .6, n = 100) # don\u0026#39;t accept reductions where information \u0026lt; .6 prt \u0026lt;- partition(df, threshold = .6) prt ## Partitioner: ## Director: Minimum Distance (Pearson) ## Metric: Intraclass Correlation ## Reducer: Scaled Mean ## ## Reduced Variables: ## 1 reduced variables created from 2 observed variables ## ## Mappings: ## reduced_var_1 = {block2_x3, block2_x4} ## ## Minimum information: ## 0.602 The partition object returned by partition() contains the reduced data.\n# return reduced data partition_scores(prt) ## # A tibble: 100 x 11 ## block1_x1 block1_x2 block1_x3 block2_x1 block2_x2 block3_x1 block3_x2 ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 -1.00 -0.344 1.35 -0.526 -1.25 1.13 0.357 ## 2 0.518 -0.434 -0.361 -1.48 -1.53 -0.317 0.290 ## 3 -1.77 -0.913 -0.722 0.122 0.224 -0.529 0.114 ## 4 -1.49 -0.998 0.189 0.149 -0.994 -0.433 0.0120 ## 5 0.616 0.0211 0.895 1.09 -1.25 0.440 -0.550 ## 6 0.0765 0.522 1.20 -0.152 -0.419 -0.912 -0.362 ## 7 1.74 0.0993 -0.654 -1.26 -0.502 -0.792 -1.03 ## 8 1.05 2.19 0.913 0.254 0.328 -1.07 -0.976 ## 9 -1.07 -0.292 -0.763 0.437 0.739 0.899 -0.342 ## 10 -1.02 -0.959 -1.33 -1.57 -1.11 0.618 0.153 ## # … with 90 more rows, and 4 more variables: block3_x3 \u0026lt;dbl\u0026gt;, ## # block3_x4 \u0026lt;dbl\u0026gt;, block3_x5 \u0026lt;dbl\u0026gt;, reduced_var_1 \u0026lt;dbl\u0026gt; You can also access the mappings of the original data to the clusters. Variables map to one and only one cluster.\n# access mapping keys mapping_key(prt) ## # A tibble: 11 x 4 ## variable mapping information indices ## \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;list\u0026gt; ## 1 block1_x1 \u0026lt;chr [1]\u0026gt; 1 \u0026lt;int [1]\u0026gt; ## 2 block1_x2 \u0026lt;chr [1]\u0026gt; 1 \u0026lt;int [1]\u0026gt; ## 3 block1_x3 \u0026lt;chr [1]\u0026gt; 1 \u0026lt;int [1]\u0026gt; ## 4 block2_x1 \u0026lt;chr [1]\u0026gt; 1 \u0026lt;int [1]\u0026gt; ## 5 block2_x2 \u0026lt;chr [1]\u0026gt; 1 \u0026lt;int [1]\u0026gt; ## 6 block3_x1 \u0026lt;chr [1]\u0026gt; 1 \u0026lt;int [1]\u0026gt; ## 7 block3_x2 \u0026lt;chr [1]\u0026gt; 1 \u0026lt;int [1]\u0026gt; ## 8 block3_x3 \u0026lt;chr [1]\u0026gt; 1 \u0026lt;int [1]\u0026gt; ## 9 block3_x4 \u0026lt;chr [1]\u0026gt; 1 \u0026lt;int [1]\u0026gt; ## 10 block3_x5 \u0026lt;chr [1]\u0026gt; 1 \u0026lt;int [1]\u0026gt; ## 11 reduced_var_1 \u0026lt;chr [2]\u0026gt; 0.602 \u0026lt;int [2]\u0026gt; unnest_mappings(prt) ## # A tibble: 12 x 4 ## variable mapping information indices ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 block1_x1 block1_x1 1 1 ## 2 block1_x2 block1_x2 1 2 ## 3 block1_x3 block1_x3 1 3 ## 4 block2_x1 block2_x1 1 4 ## 5 block2_x2 block2_x2 1 5 ## 6 block3_x1 block3_x1 1 8 ## 7 block3_x2 block3_x2 1 9 ## 8 block3_x3 block3_x3 1 10 ## 9 block3_x4 block3_x4 1 11 ## 10 block3_x5 block3_x5 1 12 ## 11 reduced_var_1 block2_x3 0.602 6 ## 12 reduced_var_1 block2_x4 0.602 7  Using partitioners partition uses an approach called Direct-Measure-Reduce to agglomerate the data: functions called partitioners tell the algorithm 1) where to look in the data 2) how to measure information loss and 3) how to reduce the data. The default partitioner that partition() uses called part_icc(), which 1) finds the closest pair of variables using a correlation-based distance matrix 2) measures information using intraclass correlation and 3) reduces acceptable clusters using scaled rowmeans.\npartition also has a number of other options. part_kmeans(), for instance, uses the K-means algorithm to find potential reductions and reduces to the minimum level of k that is still above the information threshold, measured by ICC.\n# use a lower threshold of information loss partition(df, threshold = .5, partitioner = part_kmeans()) ## Partitioner: ## Director: K-Means Clusters ## Metric: Minimum Intraclass Correlation ## Reducer: Scaled Mean ## ## Reduced Variables: ## 2 reduced variables created from 7 observed variables ## ## Mappings: ## reduced_var_1 = {block3_x1, block3_x2, block3_x5} ## reduced_var_2 = {block2_x1, block2_x2, block2_x3, block2_x4} ## ## Minimum information: ## 0.508 See the introductory vignette for more information on the built-in partitioners.\npartition() is actually agnostic to the Direct-Measure-Reduce approach used. This makes partition extremely flexible. You can edit existing partitioners or create completely new ones. For instance, if we want part_icc() to return raw row means rather than scaled row means, we can replace the reduce component of the function (here, with rowmeans()).\n# use a custom partitioner part_icc_rowmeans \u0026lt;- replace_partitioner( part_icc, reduce = as_reducer(rowMeans) ) partition(df, threshold = .6, partitioner = part_icc_rowmeans)  ## Partitioner: ## Director: Minimum Distance (Pearson) ## Metric: Intraclass Correlation ## Reducer: \u0026lt;custom reducer\u0026gt; ## ## Reduced Variables: ## 1 reduced variables created from 2 observed variables ## ## Mappings: ## reduced_var_1 = {block2_x3, block2_x4} ## ## Minimum information: ## 0.602 partition() works seamlessly with changes to the partitioner. See the vignette on extending partition for more information on customizing partitioners.\npartition also supports a number of ways to visualize partitions and permutation tests; these functions all start with plot_*(). These functions all return ggplots and can thus be extended using ggplot2.\nplot_stacked_area_clusters(df) + ggplot2::theme_minimal(14) Install partition now from CRAN or install the development version on GitHub.\n Learn More  Vignette: Introduction to partition Vignette: Extending partition partition web site GitHub Repo   ","date":1558310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558310400,"objectID":"724332975b0962560f7c690a8dac033f","permalink":"/2019/05/20/introducing-the-partition-package/","publishdate":"2019-05-20T00:00:00Z","relpermalink":"/2019/05/20/introducing-the-partition-package/","section":"post","summary":"I’m pleased to announce the CRAN release of partition 0.1.0. partition is a fast and flexible data reduction framework that minimizes information loss and creates interpretable clusters. partition uses agglomorative clustering: it starts from the ground up, matching pairs of variables and assessing the amount of information that would be explained by their reduction. If the information is above this user-specified threshold, the data is reduced. This type of reduction is particularly useful in very redundant data, such as high-resolution genetic data.","tags":[],"title":"Introducing the partition package","type":"post"},{"authors":[],"categories":null,"content":"","date":1558000800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558000800,"objectID":"e9943cdd753ca5bc8c69cad0349f2617","permalink":"/talk/2019-05-16-tidyverse-3/","publishdate":"2019-10-06T20:59:08-07:00","relpermalink":"/talk/2019-05-16-tidyverse-3/","section":"talk","summary":"Part 3 from a series at the USC Biostatistics Seminar introducing the Tidyverse. This talk introduces purrr for iteration and functional programming.","tags":[],"title":"Introduction to the Tidyverse, part 3: functional programming with purrr","type":"talk"},{"authors":["Malcolm Barrett"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":["r","rmarkdown"],"content":" TL;DR: Why should I use here?  The here package makes it easier to use sub-directories within projects It’s robust to other ways people open and run your code Like its base R cousin, file.path(), it writes paths safely across operating systems  Like a lot of people, when I learned R, I was taught to put setwd() and rm(list = ls()) at the beginning of scripts. Getting rid of any leftovers in the environment and setting the working directory so I can use relative paths made sense to me. It seemed like good practice! But setwd() and rm(list = ls()) are problematic. rm() doesn’t actually give you a clean R session; it doesn’t, for instance, detach packages. setwd(), meanwhile, is completely dependent on the way you organize your files. If you set a working directory that is an absolute path on your computer, it will only run for someone else if they rewrite the absolute path to where it is on their computer.\nLast year, Jenny Bryan shared some slides from a talk on this subject. I’ll let them speak for themselves:\n If the first line of your R script is\nsetwd(\u0026quot;C:\\Users\\jenny\\path\\that\\only\\I\\have\u0026quot;)\nI will come into your office and SET YOUR COMPUTER ON FIRE 🔥.\n  If the first line of your R script is\nrm(list = ls())\nI will come into your office and SET YOUR COMPUTER ON FIRE 🔥.\n If you haven’t read her write-up on what the issues and solutions are, you should. Here’s the basic idea:\n Use Rstudio projects. They set up a local working directory in a fresh R session, which makes it much easier for someone else to open Use here() from the here package to write file paths  Setting up a project is easy1. Projects can handle both of the problems setwd() and rm(list = ls()) are trying to solve for you. You can set it so you have a fresh R session when opening a project (either locally in the project or globally in Rstudio). Additionally, when you’re in a project, you don’t need to set your working directory. The working directory is just wherever the project is.\nSo, it may not be obvious: what’s the benefit of using the here package if projects solve both those problems?\n What’s under here? It may seem like here is just pasting paths together for you, but let’s look at what it’s actually doing. The here package is essentially a wrapper for the rprojroot package. rprojroot is a powerful tool for working with project directories, but here offers a simpler set of functions that take care of its main purpose: detecting the root directory and working with paths within it in a platform-independent way.\nIf you use here(), it will tell you your project root directory, which will look something like this.\nlibrary(here) here() ## here() starts at /Users/malcolmbarrett/folders/to/directory/ ## [1] \u0026quot;/Users/malcolmbarrett/folders/to/directory/\u0026quot; Essentially, here() is looking around for a few things that signify a root directory, like a .Rproj project file. here also has a function, set_here(), that will tag a directory as root using a .here file, even if it’s not a project. In fact, .here files take priority, then .Rproj files, followed by several other file formats (see the documentation at ?here). The last resort is the working directory. If you’re not sure why here is picking a root directory, you can ask it to explain itself using dr_here()\ndr_here() ## here() starts at /Users/malcolmbarrett/folders/to/directory/, because it contains a file matching `[.]Rproj$` with contents matching `^Version: ` in the first line here() also works a lot like file.path() in that it will create a platform-independent path for you (e.g. it will work on Windows and Mac alike). On my Mac, it looks something like this:\nhere(\u0026quot;figure\u0026quot;, \u0026quot;figure.png\u0026quot;) ## [1] \u0026quot;/Users/malcolmbarrett/folders/to/directory/figure/figure.png\u0026quot; I have a project. Why not just use relative paths? I already touched on one reason to avoid writing paths yourself: the rules aren’t necessarily the same between operating systems. You could, of course, use file.path() from base R, which safely creates a relative path for you.\nfile.path(\u0026quot;figure\u0026quot;, \u0026quot;figure.png\u0026quot;) ## [1] \u0026quot;figure/figure.png\u0026quot; But here has some added benefits: it makes it easier to manage sub-directories, and it makes your code more robust outside of projects. As an example, I’ve set up an R project on my GitHub that has a file directory like this:\nhere_here |--data |--mtcars.csv |--figs |--mpg_hp.png |--rmd |--01_read_data.Rmd |--01_read_data.pdf |--scripts |--read_data.R |--here_here.Rproj In rmd/01_read_data.Rmd, I try to call the data using a relative path from the root directory, but Rmarkdown sets a local working directory, so it fails:\nlibrary(tidyverse) read_csv(\u0026quot;data/mtcars.csv\u0026quot;) ## Error: \u0026#39;data/mtcars.csv\u0026#39; does not exist in current working directory I can solve this with some of the usual wonky backtracking, e.g. ../data/mtcars.csv, but here() will take care if it for me by finding the project directory:\nread_csv(here(\u0026quot;data\u0026quot;, \u0026quot;mtcars.csv\u0026quot;)) ## # A tibble: 32 x 11 ## mpg cyl disp hp drat wt qsec vs am gear carb ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 21 6 160 110 3.9 2.62 16.5 0 1 4 4 ## 2 21 6 160 110 3.9 2.88 17.0 0 1 4 4 ## 3 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 ## 4 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 ## 5 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 ## 6 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 ## 7 14.3 8 360 245 3.21 3.57 15.8 0 0 3 4 ## 8 24.4 4 147. 62 3.69 3.19 20 1 0 4 2 ## 9 22.8 4 141. 95 3.92 3.15 22.9 1 0 4 2 ## 10 19.2 6 168. 123 3.92 3.44 18.3 1 0 4 4 ## # … with 22 more rows It works with no trouble. Likewise, saving output to other sub-directories is no issue:\nggplot(mtcars, aes(mpg, hp)) + geom_point() ggsave(here(\u0026quot;figs\u0026quot;, \u0026quot;mpg_hp.png\u0026quot;)) Which puts it in the figs folder despite being called from the rmd folder.\n|--figs |--mpg_hp.png This is nice, as well, because if I move a file, I don’t need to change the relative directory: it works from the root up.\nAnother benefit is that, if I open any of these files outside of an Rstudio project, they will still run. For Rmarkdown files, using a relative path may be fine because it sets a local working directory when running, but .R files don’t. If you open scripts/read_data.R in a different Rstudio session, for instance, the relative path fails, but here() still works fine. That’s because it knows where the right directory is based on the .Rproj file.\nLikewise, if you or someone else sets a working directory within your project, here will still work correctly because project directories take precedence. If you need to manually change it for some reason, it’s better in this case to use set_here().\n More than a path paster here is one of the many tools in our toolkit for addressing reproducibility. Because it’s designed to work with Rstudio projects, it’s a natural tool to use within them. here is also robust to other ways people run your code. If that doesn’t convince you, you can at least sleep soundly knowing that your computer will live another day.\n   And if you’re on a Mac, you can also combine it with Alfred. Check out Hadley’s workflow with projects and Alfred here.↩\n   ","date":1541376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570320000,"objectID":"e7a287e8afb16f96a471d92366358c7c","permalink":"/2018/11/05/why-should-i-use-the-here-package-when-i-m-already-using-projects/","publishdate":"2018-11-05T00:00:00Z","relpermalink":"/2018/11/05/why-should-i-use-the-here-package-when-i-m-already-using-projects/","section":"post","summary":"TL;DR: Why should I use here?  The here package makes it easier to use sub-directories within projects It’s robust to other ways people open and run your code Like its base R cousin, file.path(), it writes paths safely across operating systems  Like a lot of people, when I learned R, I was taught to put setwd() and rm(list = ls()) at the beginning of scripts. Getting rid of any leftovers in the environment and setting the working directory so I can use relative paths made sense to me.","tags":["here"],"title":"Why should I use the here package when I'm already using projects?","type":"post"},{"authors":[],"categories":null,"content":"","date":1540461600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540461600,"objectID":"962ae1445f80bf548cb9ad38929a6895","permalink":"/talk/2018-10-25-tidyverse-2/","publishdate":"2019-10-06T20:59:08-07:00","relpermalink":"/talk/2018-10-25-tidyverse-2/","section":"talk","summary":"Part 2 from a series at the USC Biostatistics Seminar introducing the Tidyverse. This talk introduces ggplot2 for visualizing data.","tags":[],"title":"Introduction to the Tidyverse, part 2: data visualization with ggplot2","type":"talk"},{"authors":[],"categories":null,"content":"","date":1539252000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539252000,"objectID":"255e66c82d400a3e0827e0c0731411df","permalink":"/talk/2018-10-11-tidyverse-1/","publishdate":"2019-10-06T20:59:08-07:00","relpermalink":"/talk/2018-10-11-tidyverse-1/","section":"talk","summary":"Part 1 from a series at the USC Biostatistics Seminar introducing the Tidyverse. This talk introduces dplyr for manipulating data.","tags":[],"title":"Introduction to the Tidyverse, part 1: wrangling data with dplyr","type":"talk"},{"authors":[],"categories":null,"content":"","date":1539023400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539023400,"objectID":"72a7b396d29e6d6d4e5c0bdde243c615","permalink":"/talk/2018-10-08-ggplotline/","publishdate":"2019-10-06T20:41:52-07:00","relpermalink":"/talk/2018-10-08-ggplotline/","section":"talk","summary":"ggplot2 allows for beautiful out-of-the-box data visualizations. The defaults are thoughtful and work well in a lot of situations, but they will never know the story that you want to tell with your plot. I’ll talk about a few ways to use ggplot2 and extension packages to guide your story. Using a case study, I’ll cover some built-in tools to use color, themes, and direct labeling to highlight important parts of your plot. I’ll then talk about two packages, ggrepel and gghighlight, that make it easy to add labels and colors in a way that focuses the reader's attention. Finally, I’ll talk about arranging and annotating plots with some tools in ggplot2, cowplot, and patchwork to develop your narrative.","tags":[],"title":"ggplotline: telling a story with labels, colors, and layout","type":"talk"},{"authors":[],"categories":null,"content":"","date":1534154400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534154400,"objectID":"9fbf0625c69b1ad351cd196c9324fcda","permalink":"/talk/2018-08-13-usc-rbootcamp/","publishdate":"2019-10-06T20:29:10-07:00","relpermalink":"/talk/2018-08-13-usc-rbootcamp/","section":"talk","summary":"A week-long, intensive introduction to R for students and faculty of the Department of Preventive Medicine at the University of Southern California. This workshop covers R basics, the Tidyverse, R Markdown, and statistical computing.","tags":[],"title":"USC Preventive Medicine R Bootcamp","type":"talk"},{"authors":[],"categories":["r","causalinference"],"content":"  Last week, I presented ggdag at JSM in Vancouver. As you can imagine, I had a lot of conversations with people about DAGs, confounding, colliders, and all the types of bias that can arise in research. One strange type of bias came up a couple of times that I don’t see discussed very often: measuring either the effect you are studying (x) or a variable along a confounding pathway (z) incorrectly can make it appear as if there is an interaction between x and z, even if there isn’t one.\nLet’s consider a simple example: there’s an association between x and y, which is what we’re interested in, and this association is confounded by z. This is a classic example of confounding.\nlibrary(broom) library(tidyverse) library(kableExtra) # to add headers on kable() tables library(knitr) library(ggdag) library(patchwork) # to combine the plots options(knitr.kable.NA = \u0026quot;--\u0026quot;) # don\u0026#39;t show NA values in kable() set.seed(293951) confounder_triangle(x_y_associated = TRUE) %\u0026gt;% ggdag() + theme_dag() Let’s simulate some data. x and y are both continuous, and z is binary (0 or 1, with only about 10% of the population with z=1). We’ll simulate 10,000 participants so random error is not a big issue.\nz \u0026lt;- sample(0:1, 10000, replace = TRUE, prob = c(.9, .1)) x \u0026lt;- rnorm(10000) + 2*z y \u0026lt;- 1 + 2*x + 1.5*z + rnorm(10000) df \u0026lt;- data_frame(y, x, z) ## Warning: `data_frame()` is deprecated, use `tibble()`. ## This warning is displayed once per session. df ## # A tibble: 10,000 x 3 ## y x z ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 -0.494 -0.463 0 ## 2 -0.649 0.0242 0 ## 3 2.50 -0.146 0 ## 4 1.36 0.0434 1 ## 5 3.41 1.00 0 ## 6 -2.17 -1.94 0 ## 7 0.989 0.209 0 ## 8 0.0667 -0.831 0 ## 9 1.98 0.0196 0 ## 10 1.45 0.0405 0 ## # … with 9,990 more rows We’ll consider what happens when we mismeasure x or z. For each scenario, we’ll compare models with and without an interaction term (x*z). To fit, tidy, and compare the models, let’s write a few functions:\n tidy_lm() tidies the regression models and formats the numbers a little models_kable() joins two tables and renders them with kable() using the kableExtra package to add a couple of headers compare_models() combines these two functions and accepts the variables we want to change as arguments so we don’t need to rewrite formulas every time plot_models() plots the relationship between x and y by levels of z with hex bins (for the distribution) and regression lines (to assess interaction), combining them with the patchwork and cowplot packages (plus a little tidy eval magic!)  tidy_lm \u0026lt;- function(model) { model %\u0026gt;% tidy() %\u0026gt;% mutate(p.value = ifelse(p.value \u0026lt; .001, \u0026quot;\u0026lt;.001\u0026quot;, round(p.value, 3))) %\u0026gt;% mutate_if(is.numeric, round, 2) } models_kable \u0026lt;- function(no_int, int, ...) { no_int \u0026lt;- select(no_int, term, estimate, p.value) int \u0026lt;- select(int, term, estimate, p.value) full_join(no_int, int, by = \u0026quot;term\u0026quot;) %\u0026gt;% mutate(term = c(\u0026quot;(Intercept)\u0026quot;, \u0026quot;x\u0026quot;, \u0026quot;z\u0026quot;, \u0026quot;x * z\u0026quot;)) %\u0026gt;% kable(col.names = c(\u0026quot;Term\u0026quot;, rep(c(\u0026quot;Estimate\u0026quot;, \u0026quot;P-Value\u0026quot;), 2)), ...) %\u0026gt;% add_header_above(c(\u0026quot; \u0026quot;, \u0026quot;No Interaction\u0026quot; = 2, \u0026quot;Interaction\u0026quot; = 2)) } compare_models \u0026lt;- function(exposure = \u0026quot;x\u0026quot;, confounder = \u0026quot;z\u0026quot;, ...) { fmla \u0026lt;- as.formula(paste(\u0026quot;y ~ \u0026quot;, exposure, \u0026quot; + \u0026quot;, confounder)) # create a formula no_int \u0026lt;- lm(fmla, data = df) %\u0026gt;% tidy_lm() fmla_int \u0026lt;- fmla \u0026lt;- as.formula(paste(\u0026quot;y ~ \u0026quot;, exposure, \u0026quot; * \u0026quot;, confounder)) int \u0026lt;- lm(fmla_int, data = df) %\u0026gt;% tidy_lm() models_kable(no_int, int, ...) } plot_models \u0026lt;- function(x = x, z = z, x_label = \u0026quot;x (Measured well)\u0026quot;, z_label = \u0026quot;Confounder (Measured well)\u0026quot;, crude = FALSE) { x \u0026lt;- rlang::enquo(x) z \u0026lt;- rlang::enquo(z) p1 \u0026lt;- df %\u0026gt;% ggplot(aes(x = !!x, y = y, col = factor(!!z))) + geom_hex(aes(fill = factor(z)), col = \u0026quot;white\u0026quot;, alpha = .7) + scale_color_manual(name = z_label, values = c(\u0026quot;#56B4E9\u0026quot;, \u0026quot;#EFA722\u0026quot;, \u0026quot;#E36A25\u0026quot;)) + scale_fill_manual(name = z_label, values = c(\u0026quot;#56B4E9\u0026quot;, \u0026quot;#EFA722\u0026quot;, \u0026quot;#E36A25\u0026quot;)) + theme_minimal(base_size = 14) + theme(legend.position = \u0026quot;bottom\u0026quot;, axis.title.x = element_blank()) + labs(y = \u0026quot;y (Measured well)\u0026quot;) + ylim(c(-10, 16)) legend \u0026lt;- cowplot::get_legend(p1) p1a \u0026lt;- p1 + theme(legend.position = \u0026quot;none\u0026quot;) p2 \u0026lt;- df %\u0026gt;% ggplot(aes(x = !!x, y = y, col = factor(!!z))) + geom_hex(fill = \u0026quot;grey92\u0026quot;, col = \u0026quot;white\u0026quot;, alpha = .8) + geom_smooth(method = \u0026quot;lm\u0026quot;, se = FALSE, size = 1) + scale_color_manual(name = z_label, values = c(\u0026quot;#56B4E9\u0026quot;, \u0026quot;#EFA722\u0026quot;, \u0026quot;#E36A25\u0026quot;)) + theme_minimal(base_size = 14) + theme(legend.position = \u0026quot;none\u0026quot;, axis.title = element_blank()) + ylim(c(-10, 16)) if (crude) { p2 \u0026lt;- p2 + geom_smooth(aes(group = 1, col = \u0026quot;Crude Estimate\u0026quot;), method = \u0026quot;lm\u0026quot;, se = FALSE, size = 1) + theme(legend.position = \u0026quot;bottom\u0026quot;) legend \u0026lt;- cowplot::get_legend(p2) p2 \u0026lt;- p2 + theme(legend.position = \u0026quot;none\u0026quot;) } patchworked \u0026lt;- p1a + p2 pl1 \u0026lt;- cowplot::plot_grid(patchworked, align = \u0026quot;h\u0026quot;) pl2 \u0026lt;- cowplot::add_sub(pl1, x_label, vpadding = grid::unit(0,\u0026quot;lines\u0026quot;), y = 4.5, x = .5, vjust = 4.5) pl3 \u0026lt;- cowplot::plot_grid(NULL, legend, NULL, nrow = 1) cowplot::plot_grid(pl2, pl3, ncol = 1, rel_heights = c(1.5, .2)) } If all three variables are measured well, there’s no problem. The effect estimates are about right, and there’s no false interaction. For the plot, notice how 1) the crude estimate (which ignores z) is an mix of the other two lines and 2) the slopes of the lines for z=0 and z=1 are parallel.\ncompare_models()     No Interaction   Interaction     Term  Estimate  P-Value  Estimate  P-Value      (Intercept)  0.99  \u0026lt;.001  0.99  \u0026lt;.001    x  2.01  \u0026lt;.001  2.02  \u0026lt;.001    z  1.46  \u0026lt;.001  1.47  \u0026lt;.001    x * z  –  –  0.00  0.912     plot_models(crude = TRUE) ## Warning: Computation failed in `stat_binhex()`: ## Package `hexbin` required for `stat_binhex`. ## Please install and try again. ## Warning: Computation failed in `stat_binhex()`: ## Package `hexbin` required for `stat_binhex`. ## Please install and try again. ## Warning: Computation failed in `stat_binhex()`: ## Package `hexbin` required for `stat_binhex`. ## Please install and try again. Measurement error for the outcome (x) Let’s say that, in practice, we are measuring x incorrectly (measurement error). The only device around to measure x is an old 1980 version of the measureator, which by complete coincidence is the same year Sander Greenland first talked about this issue. The 1980 version of this device has a problem: it measures x better in people who have values of y above 0 than those below 0. The 2018 version of the device still measures x with error, but the manufacturer fixed the problem with y, and now it has nothing to do with that.\nMismeasured x, dependent on y When we use x_1980 as a proxy for x, there now appears to be an interaction with z. It’s present in both the model estimates and the plot. The slopes are no longer parallel. The effect of x on y seems like it’s heterogeneous for levels of z: for people with z=1, the effect of x on y is stronger.\nmeasureator2018 \u0026lt;- function(x) x + rnorm(10000) measureator1980 \u0026lt;- function(x) ifelse(y \u0026gt; 2, x + rnorm(10000, sd = 1.5), ifelse(y \u0026gt; 0, x + rnorm(10000, sd = 2), x + rnorm(10000, sd = 3))) df \u0026lt;- df %\u0026gt;% mutate( x_2018 = measureator2018(x), x_1980 = measureator1980(x) ) compare_models(\u0026quot;x_1980\u0026quot;)     No Interaction   Interaction     Term  Estimate  P-Value  Estimate  P-Value      (Intercept)  0.98  \u0026lt;.001  0.98  \u0026lt;.001    x  0.34  \u0026lt;.001  0.32  \u0026lt;.001    z  4.80  \u0026lt;.001  4.28  \u0026lt;.001    x * z  –  –  0.29  \u0026lt;.001     plot_models(x = x_1980, x_label = \u0026quot;Mismeasured x (1980 device)\u0026quot;) ## Warning: Computation failed in `stat_binhex()`: ## Package `hexbin` required for `stat_binhex`. ## Please install and try again. ## Warning: Computation failed in `stat_binhex()`: ## Package `hexbin` required for `stat_binhex`. ## Please install and try again. What’s going on here? The plot helps demonstrate the distortion in x (it’s much less stable when y is below 0 and most people with z=1 are above 0). We can put this together in a DAG. In the case of the 1980 device, we have differential measurement error, so called because the degree of mismeasurement is affected by y. We want to estimate the effect of x, but what we really have is an approximation, x_m, dependent on both error (the amount of mismeasurement) and x. error is dependent on y.\ndme \u0026lt;- dagify(y ~ x + z, x ~ z, x_m ~ x + error, error ~ y) ggdag(dme) + theme_dag() Because two arrows are leading into it, x_m is a collider. Including a collider in the regression model will induce an association between its parents, creating bias. Collider bias can also travel upstream (see the previous link), here affecting z and y, as well. Look at the web of associations it creates!\nggdag_adjust(dme, c(\u0026quot;x_m\u0026quot;, \u0026quot;z\u0026quot;)) + theme_dag() + scale_color_manual(values = c(\u0026quot;#2C7FBF\u0026quot;, \u0026quot;#E69F00\u0026quot;)) + ggraph::scale_edge_color_manual(values = c(\u0026quot;#E7E7E7\u0026quot;, \u0026quot;#E69F00\u0026quot;))  Mismeasured x, independent of y Things are a little different if we use the 2018 device, which measures x incorrectly but has nothing to do with y. In this case, there’s no apparent interaction.\ncompare_models(\u0026quot;x_2018\u0026quot;)     No Interaction   Interaction     Term  Estimate  P-Value  Estimate  P-Value      (Intercept)  0.97  \u0026lt;.001  0.97  \u0026lt;.001    x  0.99  \u0026lt;.001  0.99  \u0026lt;.001    z  3.48  \u0026lt;.001  3.40  \u0026lt;.001    x * z  –  –  0.04  0.27     plot_models(x = x_2018, x_label = \u0026quot;Mismeasured x (1980 device)\u0026quot;) ## Warning: Computation failed in `stat_binhex()`: ## Package `hexbin` required for `stat_binhex`. ## Please install and try again. ## Warning: Computation failed in `stat_binhex()`: ## Package `hexbin` required for `stat_binhex`. ## Please install and try again. This is a case of non-differential measurement error; the error is not dependent on the outcome, y. x_m is still a collider but it ends up being less important. Instead, the model underestimate the effect of x on y (otherwise known as bias towards the null, which is what happens under most cases of non-differential error) and attributes too much of the effect on y to z (x is a mediator of z’s impact on y).\nndme \u0026lt;- dagify(y ~ x + z, x ~ z, x_m ~ x + error) ggdag(ndme) + theme_dag()   Measurement error for covariates (z) What about if we measure x well but mismeasure the confounder, z? Let’s say we have a similar situation: one device that occasionally misclassifies z but isn’t effected by y and another, broken device that measures z in people who have higher values of y better.\nswap \u0026lt;- function(x) ifelse(x == 1, 0, 1) z_ometer \u0026lt;- function(z) { mismeasured \u0026lt;- sample(c(TRUE, FALSE), size = 10000, replace = TRUE, prob = c(.3, .7)) ifelse(mismeasured, swap(z), z) } z_ometer_broken \u0026lt;- function(z) { mismeasured \u0026lt;- ifelse( df$y \u0026gt; 0, sample(c(TRUE, FALSE), size = 10000, replace = TRUE, prob = c(.5, .5)), sample(c(TRUE, FALSE), size = 10000, replace = TRUE, prob = c(.1, .9)) ) ifelse(mismeasured, swap(z), z) } df \u0026lt;- df %\u0026gt;% mutate( z_results = z_ometer(z), z_results_broken = z_ometer_broken(z) ) The DAGs for z_m look pretty similar to x_m.\nndme_z \u0026lt;- dagify(y ~ x + z, x ~ z, z_m ~ z + error) dme_z \u0026lt;- dagify(y ~ x + z, x ~ z, z_m ~ z + error, error ~ y) dag1 \u0026lt;- ggdag(ndme_z) + theme_dag() + ggtitle(\u0026quot;Wrongly measure z, independent of y\u0026quot;) dag2 \u0026lt;- ggdag(dme_z) + theme_dag() + ggtitle(\u0026quot;Wrongly measure z, dependent on y\u0026quot;) dag1 + dag2 For both devices, there appears to be a little interaction (probably only detectable because of our large sample size). While that’s often the case, the larger issue is that our proxies for z are no longer blocking the back-door path between x and y. In other words, there’s residual confounding. In this example, what ends up happening is a little bit of false interaction for both differential and non-differential misclassification of z and a lot of confounding bias. The estimate for x is a little bit off (about 10% too high), and z is much too low (it’s biased towards the null).\nMismeasured z, dependent on y compare_models(confounder = \u0026quot;z_results_broken\u0026quot;)     No Interaction   Interaction     Term  Estimate  P-Value  Estimate  P-Value      (Intercept)  1.02  \u0026lt;.001  1.02  \u0026lt;.001    x  2.19  \u0026lt;.001  2.22  \u0026lt;.001    z  0.23  \u0026lt;.001  0.26  \u0026lt;.001    x * z  –  –  -0.08  \u0026lt;.001     plot_models(z = z_results_broken, z_label = \u0026quot;Confounder (Z-ometer, broken)\u0026quot;) ## Warning: Computation failed in `stat_binhex()`: ## Package `hexbin` required for `stat_binhex`. ## Please install and try again. ## Warning: Computation failed in `stat_binhex()`: ## Package `hexbin` required for `stat_binhex`. ## Please install and try again.  Mismeasured z, independent of y compare_models(confounder = \u0026quot;z_results\u0026quot;)     No Interaction   Interaction     Term  Estimate  P-Value  Estimate  P-Value      (Intercept)  1.03  \u0026lt;.001  1.04  \u0026lt;.001    x  2.21  \u0026lt;.001  2.13  \u0026lt;.001    z  0.21  \u0026lt;.001  0.16  \u0026lt;.001    x * z  –  –  0.19  \u0026lt;.001     plot_models(z = z_results, z_label = \u0026quot;Confounder (Z-ometer, broken)\u0026quot;) ## Warning: Computation failed in `stat_binhex()`: ## Package `hexbin` required for `stat_binhex`. ## Please install and try again. ## Warning: Computation failed in `stat_binhex()`: ## Package `hexbin` required for `stat_binhex`. ## Please install and try again. So, mismeasurement of both x and z can cause problems. Under most circumstances, of course, we are mismeasuring more than one variable (including y!). Moreover, the errors in the way those variables are measured may themselves be dependent. We need to, then, be very mindful of the structures of these bias and, if necessary, try to address them with bias analysis approaches.\nIf you want to learn about more about these methods, you may be interested in this great-looking resource from Maarten van Smeden:\n{{\u0026lt; tweet 1025990407757422592 \u0026gt;}}\nThanks to him for providing it!\n  ","date":1533340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533340800,"objectID":"d1031415267994c17d7527e13066d32c","permalink":"/2018/08/04/when-interaction-is-not-interaction-confounding-and-measurement-error/","publishdate":"2018-08-04T00:00:00Z","relpermalink":"/2018/08/04/when-interaction-is-not-interaction-confounding-and-measurement-error/","section":"post","summary":"Last week, I presented ggdag at JSM in Vancouver. As you can imagine, I had a lot of conversations with people about DAGs, confounding, colliders, and all the types of bias that can arise in research. One strange type of bias came up a couple of times that I don’t see discussed very often: measuring either the effect you are studying (x) or a variable along a confounding pathway (z) incorrectly can make it appear as if there is an interaction between x and z, even if there isn’t one.","tags":["epidemiology"],"title":"When interaction is not interaction: confounding and measurement error","type":"post"},{"authors":[],"categories":["r","dataviz","releases","causalinference"],"content":" I’m pleased to announce the release of ggdag 0.1.0 on CRAN! ggdag uses the powerful dagitty package to create and analyze structural causal models and plot them using ggplot2 and ggraph in a tidy, consistent, and easy manner. You can use dagitty objects directly in ggdag, but ggdag also includes wrappers to make DAGs using a more R-like syntax:\n# install.packages(\u0026quot;ggdag\u0026quot;) library(ggdag) dag \u0026lt;- dagify(y ~ x + z, x ~ z) %\u0026gt;% tidy_dagitty() dag ## # A DAG with 3 nodes and 3 edges ## # ## # A tibble: 4 x 8 ## name x y direction to xend yend circular ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; ## 1 x 8.21 7.43 -\u0026gt; y 7.57 8.19 FALSE ## 2 z 8.55 8.37 -\u0026gt; x 8.21 7.43 FALSE ## 3 z 8.55 8.37 -\u0026gt; y 7.57 8.19 FALSE ## 4 y 7.57 8.19 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; NA NA FALSE ggdag(dag) You can learn more about it on the ggdag website, a pkgdown site that includes rendered documentation and the following vignettes:\n An Introduction to ggdag An Introduction to Directed Acyclic Graphs Common Structures of Bias  Give it a try, and please file an bugs or suggestions to the GitHub repo.\nI also want to thank a few of my fellow USC PhDs, David Bogumil, Ugonna Ihenacho, and Zhi Yang, for helping me polish the articles and offering helpful suggestions on some of the aesthetic details of ggdag. Thanks, y’all!\n","date":1522195200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522195200,"objectID":"a3753b81cd3de2e9c5b46871a1830511","permalink":"/2018/03/28/ggdag-0.1.0/","publishdate":"2018-03-28T00:00:00Z","relpermalink":"/2018/03/28/ggdag-0.1.0/","section":"post","summary":"I’m pleased to announce the release of ggdag 0.1.0 on CRAN! ggdag uses the powerful dagitty package to create and analyze structural causal models and plot them using ggplot2 and ggraph in a tidy, consistent, and easy manner. You can use dagitty objects directly in ggdag, but ggdag also includes wrappers to make DAGs using a more R-like syntax:\n# install.packages(\u0026quot;ggdag\u0026quot;) library(ggdag) dag \u0026lt;- dagify(y ~ x + z, x ~ z) %\u0026gt;% tidy_dagitty() dag ## # A DAG with 3 nodes and 3 edges ## # ## # A tibble: 4 x 8 ## name x y direction to xend yend circular ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; ## 1 x 8.","tags":["ggdag"],"title":"ggdag 0.1.0","type":"post"},{"authors":[],"categories":["r"],"content":"\n\n\n\n","date":1519898400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569522600,"objectID":"de241333a75864d841f60484004be7eb","permalink":"/talk/metaanalysis-viz/","publishdate":"2018-03-01T10:00:00Z","relpermalink":"/talk/metaanalysis-viz/","section":"talk","summary":"An introduction to data visualization for meta-anlyses using ggplot2 and the tidymeta package.","tags":[],"title":"Introduction to Data Visualization for Meta-Analysis","type":"talk"},{"authors":[],"categories":["r"],"content":" Update with markovifyR Thanks to Maëlle Salmon, who referred me to this post by Julia Silge and Nick Larsen, I explored doing this using the markovifyR package, and the results are unbelievable. See the bottom of the post for an updated batch of sonnets!\n Original post I recently saw Katie Jolly’s post, in which she produced Rupi Kuar-style poems using Markov Chains in R. I absolutely loved it, so I decided to try it with Shakespeare’s 154 sonnets using her post as a skeleton.\n Downloading and cleaning the sonnets In addition to markovchain and tidyverse, I’m going to use the gutenberger package to download the sonnets.\nlibrary(gutenbergr) library(tidyverse) library(markovchain)  shakespeare \u0026lt;- gutenberg_works(title == \u0026quot;Shakespeare\u0026#39;s Sonnets\u0026quot;) %\u0026gt;% pull(gutenberg_id) %\u0026gt;% gutenberg_download(verbose = FALSE) shakespeare ## # A tibble: 2,625 x 2 ## gutenberg_id text ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; ## 1 1041 THE SONNETS ## 2 1041 \u0026quot;\u0026quot; ## 3 1041 by William Shakespeare ## 4 1041 \u0026quot;\u0026quot; ## 5 1041 \u0026quot;\u0026quot; ## 6 1041 \u0026quot;\u0026quot; ## 7 1041 \u0026quot;\u0026quot; ## 8 1041 \u0026quot; I\u0026quot; ## 9 1041 \u0026quot;\u0026quot; ## 10 1041 \u0026quot; From fairest creatures we desire increase,\u0026quot; ## # … with 2,615 more rows Because the sonnets are in gutenberger, they’re already in a nice format to work with. I just need to do a little cleaning up: like Katie, I removed the punctuation, but I also have to clear out the sonnet titles, which were Roman numerals, and some title info.\n# a little function to make life easier `%not_in%` \u0026lt;- function(lhs, rhs) { !(lhs %in% rhs) } # remove new lines symbol, sonnet Roman numerals, and punctation # and split into vector bills_words \u0026lt;- shakespeare %\u0026gt;% mutate(text = text %\u0026gt;% str_trim() %\u0026gt;% str_replace_all(\u0026quot;--\u0026quot;, \u0026quot; \u0026quot;) %\u0026gt;% str_replace_all(\u0026quot;[^[:alnum:][:space:]\u0026#39;]\u0026quot;, \u0026quot;\u0026quot;) %\u0026gt;% str_replace_all(\u0026quot;^M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$\u0026quot;, \u0026quot;\u0026quot;) %\u0026gt;% str_to_lower()) %\u0026gt;% filter(text %not_in% c(\u0026quot;the sonnets\u0026quot;, \u0026quot;by william shakespeare\u0026quot;, \u0026quot;\u0026quot;, \u0026quot; \u0026quot;)) %\u0026gt;% pull(text) %\u0026gt;% str_split(\u0026quot; \u0026quot;) %\u0026gt;% unlist()  I’m also going to extract the punctuation and assess how many of each there are for when I actually assemble the sonnets later.\npunctuation \u0026lt;- shakespeare %\u0026gt;% pull(text) %\u0026gt;% str_extract_all(\u0026quot;[^[:alnum:][:space:]\u0026#39;]\u0026quot;) %\u0026gt;% unlist() punctuation_probs \u0026lt;- punctuation[punctuation %not_in% c(\u0026quot;-\u0026quot;, \u0026quot;(\u0026quot;, \u0026quot;)\u0026quot;)] %\u0026gt;% table() %\u0026gt;% prop.table()  Fit the Markov Chain Now fit the Markov Chain with the vector of words.\n# fit a Markov Chain sonnet_chain \u0026lt;- markovchainFit(bills_words) cat(markovchainSequence(n = 10, markovchain = sonnet_chain$estimate), collapse = \u0026quot; \u0026quot;) i can afford no longer nurseth the joy my poverty\nAnd finally, here are a few functions to piece together lines to make them look like a sonnet using the walk() function from purrr to print out the lines (since it’s a side effect). No, they’re not actually iambic pentameter :(\nwrite_a_line \u0026lt;- function(n_lines = 1) { walk(1:n_lines, function(.x) { # put together lines of more or less average length lines \u0026lt;- markovchainSequence(n = sample(c(6:9), 1), markovchain = sonnet_chain$estimate) %\u0026gt;% paste(collapse = \u0026quot; \u0026quot;) # add end-of-line punctuation based on their occurence end_punctuation \u0026lt;- ifelse(.x == n_lines, \u0026quot;.\u0026quot;, sample(names(punctuation_probs), size = 1, prob = punctuation_probs)) cat(paste0(lines, end_punctuation, \u0026quot; \\n\u0026quot;)) }) } psuedosonnet \u0026lt;- function() { walk(1:3, function(.x) { write_a_line(4) cat(\u0026quot; \\n\u0026quot;) }) write_a_line(2) }  Generating the sonnets Let’s try it out.\nPsuedosonnet 1: set.seed(154) psuedosonnet() wing and folly doctorlike controlling skill who knows the,\nagain assur’d and given grace but when all date:\nshook three till my mind these blenches,\nin sweetest things deem’d not so dear delight.\nthe mouths of love swearing in these,\nand soon to woe compar’d with;\nat your bounty cherish she is ’greeing and yet,\nby lies buried age and simple savour pitiful thrivers.\nam now with intelligence as the stage presenteth nought,\nwhich it for me suffering my?\ndays making lascivious grace is not thy days are:\nexpired for myself and tombs of my.\nright my heart another gay why dost,\nof my use your self thou.\n Psuedosonnet 2: psuedosonnet() past reason hunted and checked with thee;\nor changes right or at least so strong,\nthe ashes of glass his prescriptions,\nhim to decay lest eyes be false borrowed face.\nand all kinds of love her blood that,\no none but in these offices so.\nand his spring within the view!\nto weigh how thy book of.\nby a tomb of public means which,\nor who moving points on to?\ncan yet do see my state and age unbred?\nmuse brings forth the orient when.\nthy hair the world away to show thee:\none by day when in love things.\n Psuedosonnet 3: psuedosonnet() or else receiv’st not fade die but.\ngrief to mend to stay whilst;\nreeks i hold thee devouring time that,\nwith you pattern of such day my love happy.\nhate me last so bright who leaves thy beauty.\neach friend and your true that one that,\nwhilst like a vengeful canker blooms?\nand therefore my bad thus to.\nshould i not counted fair whose,\nstate but day arising from hands to;\nof flower with his presence grace impiety,\nonly herald to the beast that plea.\neyelids open wide in posterity thou send’st from,\nnor shall have err’d and kept unused.\n Psuedosonnet 4: psuedosonnet() that pine to my home of thine own,\nlove being crown’d with that man’s;\nwhat need’st thou usurer why of this thy love’s,\nwith sovereign cure i question make him that i.\nto the ear confounds him have drawn,\nfight and beauty should my muse and strangely.\nliv’d alone stands but one so,\nwhilst i alone o in the lines that brightness.\ncreature the lease dost wake elsewhere but me;\nevil tempteth my love that which still,\ni am attainted that in the thing to.\nit thy sweet deaths be but those.\nthat our appetite which he is hanging still:\ninhabit on death to make one.\n Psuedosonnet 5: psuedosonnet() and confounds in me not my?\nalchemy to all my comfort now i send,\nthe pleasure thine own desert and no hatred in,\nwith disdain lest guilty goddess go since.\nseem right myself i’ll fight after you,\nwane so love and simple truth.\nah do dispense you like of time despite,\nto store harsh featureless and by.\nmy reason is daily new lo,\nlove thee i’ll run and in themselves forsake me;\nthou through windows to play as ’twixt a,\nme both and to love of.\nstill my bonds in disgrace with newer:\nmorrow to slavery my side against the weary.\n  Update Alright, this time I’m going to try it with the markovifyR package. I’m basically going to do the same cleaning as above, but this time I’ll be putting entire sentences, punctuation and all, into the Markov model. The markovify_text() function also accepts start words, so I thought it might look good to start with a sample of 100 starting words from the sonnets and construct the lines from there.\nlibrary(markovifyR) # same as above, but maintain as sentences and keep punctuation bills_sentences \u0026lt;- shakespeare %\u0026gt;% mutate(text = text %\u0026gt;% str_trim() %\u0026gt;% str_replace_all(\u0026quot;--\u0026quot;, \u0026quot; \u0026quot;) %\u0026gt;% str_replace_all(\u0026quot;^M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$\u0026quot;, \u0026quot;\u0026quot;) %\u0026gt;% str_to_lower()) %\u0026gt;% filter(text %not_in% c(\u0026quot;the sonnets\u0026quot;, \u0026quot;by william shakespeare\u0026quot;, \u0026quot;\u0026quot;, \u0026quot; \u0026quot;)) # fit the Markov Chain markovify_model \u0026lt;- generate_markovify_model( input_text = bills_sentences$text, markov_state_size = 2L, max_overlap_total = 25, max_overlap_ratio = .85 ) # generate a sonnet markovify_sonnet \u0026lt;- function() { lines \u0026lt;- markovify_text( markov_model = markovify_model, maximum_sentence_length = 75, output_column_name = \u0026#39;sonnet_line\u0026#39;, count = 50, tries = 1000, start_words = sample(generate_start_words(markovify_model)$wordStart, 100), only_distinct = TRUE, return_message = FALSE) %\u0026gt;% filter(str_count(sonnet_line, \u0026quot;\\\\w+\u0026quot;) \u0026gt; 5 \u0026amp; str_count(sonnet_line, \u0026quot;\\\\w+\u0026quot;) \u0026lt; 10) %\u0026gt;% slice(sample(1:n(), 14)) %\u0026gt;% mutate(id = 1:n()) %\u0026gt;% select(id, sonnet_line) # add a period to the last line if the last charachter isn\u0026#39;t punctuation # that ends a sentence last_line \u0026lt;- lines[lines$id == 14, \u0026quot;sonnet_line\u0026quot;] lines[lines$id == 14, \u0026quot;sonnet_line\u0026quot;] \u0026lt;- str_replace(last_line, \u0026quot;.$(?\u0026lt;!//.//!//?|[:alnum:])\u0026quot;, \u0026quot;.\u0026quot;) # print in a sonnet-like format walk(1:14, function(.x) { cat(lines$sonnet_line[.x], \u0026quot; \\n\u0026quot;) # add a space every four lines if (.x %% 4 == 0) cat(\u0026quot;\\n\u0026quot;) }) } Markovify Sonnet 1: markovify_sonnet() now counting best to be invited\nthen, in the breath of words respect,\nare windows to my dear time’s waste:\nspeak of that which thou departest;\nspeak of my faults thy sweet graces graced be;\nreserve them for my sin you did impute,\nthen, beauteous niggard, why dost thou to mine eyes,\ndie to themselves. sweet roses do not kill\nthis silence for my love, yea take them all;\nare windows to my thoughts as food to life,\nbeauty’s effect with beauty of thy days.\nthere is but as the rich, whose blessed key,\nthis were to be cross’d:\nnot from the book of honour razed quite.\n Markovify Sonnet 2: markovify_sonnet() thine eyes i love, and look strange;\nhaply i think my love alone.\nwhich should transport me farthest from your tongue;\nentitled in thy power to lend base subjects light?\nwishing me like to the very same;\nnot that the world’s fresh ornament,\nlooking with pretty ruth upon my brow;\nof faults conceal’d, wherein i am forsworn,\ntoo base of thee this i do change:\nnow all is done, save what is told.\nto this composed wonder of your fame!\ndisdains the tillage of thy days.\ntill i return, of posting is no remedy,\nnot wondering at the present nor the prophetic soul\n Markovify Sonnet 3: markovify_sonnet() want nothing that the world an end,\nkill me with that which it doth catch;\nwithin the knowledge of mine own love’s might.\no! change thy thought, that i prove,\nwithin the level of your love.\npresume not on thy humour doth depend:\nere you were once unkind befriends me now,\nwhen sometime lofty towers i see thee,\nwishing me like to the world enjoys it;\nwho taught thee how thy precious minutes waste;\nmad in pursuit of the time,\nhave added feathers to the time,\nthe summer’s flower is to render thee.\nwhat merit do i not to be invited\n Markovify Sonnet 4: markovify_sonnet() o, no! it is a man right fair,\nby seeing farther than my barren rhyme?\neven that your love taught it this alchemy,\ntheirs for their habitation chose out thee,\njust to the edge of doom.\nfull many a holy and obsequious tear\ntwo loves i have frequent been with unknown minds,\nhow have mine eyes be blessed made\nspeak of that which still doth grow?\nkind is my love’s breath? the purple pride\nhim in thy steel bosom’s ward,\neven so, being full of blame,\nspeak of the dead, which now appear\nwill be true despite thy scythe and crooked knife.\n Markovify Sonnet 5: markovify_sonnet() never believe though in thy creation did decree\nmake but my five senses can\nbut all alone beweep my outcast state,\nalack! what poverty my muse want subject to invent,\nbut wherefore do not do the thing they see;\nlo! in the number let me be borne alone.\nsteal from his low tract, and look strange;\nso, either by thy beauty being mute,\nbetwixt mine eye loves it and doth first begin.\nwhence hast thou this becoming of their excellence.\ndost thou to mine own worth do define,\nwretched in this line some interest,\nyet, do thy worst to steal thyself away,\nalthough thou steal thy sweet self prove.\nWell, call me Shockedspeare.\nExit, pursued by a bear\n  ","date":1516924800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516924800,"objectID":"dfa3f9c0f33699ac814c7eb6d13161c8","permalink":"/2018/01/26/stochastic-shakespeare-sonnets-produced-by-markov-chains-in-r/","publishdate":"2018-01-26T00:00:00Z","relpermalink":"/2018/01/26/stochastic-shakespeare-sonnets-produced-by-markov-chains-in-r/","section":"post","summary":"Update with markovifyR Thanks to Maëlle Salmon, who referred me to this post by Julia Silge and Nick Larsen, I explored doing this using the markovifyR package, and the results are unbelievable. See the bottom of the post for an updated batch of sonnets!\n Original post I recently saw Katie Jolly’s post, in which she produced Rupi Kuar-style poems using Markov Chains in R. I absolutely loved it, so I decided to try it with Shakespeare’s 154 sonnets using her post as a skeleton.","tags":["textanalysis"],"title":"Stochastic Shakespeare: Sonnets Produced by Markov Chains in R","type":"post"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Malcolm Barrett","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Malcolm Barrett","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]